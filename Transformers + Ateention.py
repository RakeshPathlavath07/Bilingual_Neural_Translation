# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vbfPKY8NoZUFFMDqu8RPFVBBW8UoVgE0
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from sklearn.utils import shuffle
from wordcloud import WordCloud
# %matplotlib inline
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense,Activation,Lambda,Input,Embedding,LSTM

df=pd.read_csv('/content/hindi_english_parallel.csv')
print(df.head())

print(len(df))

# Dropping null values and resetting the index
df.dropna(inplace=True)
df=df.reset_index(drop=True)

df.isnull().sum()

len(df)

df['english']

import re

def is_clean_sentence(en, hi):
    # Lowercase English for pattern matching
    en_l = en.lower()
    hi_l = hi.lower()

    # ‚ùå patterns that indicate PO / UI noise
    noise_patterns = [
        r'gnome', r'gtk', r'lib', r'\.po',
        r'#-#', r'args', r'%s', r'%d',
        r'\{.*?\}', r'\[.*?\]'
    ]

    for pat in noise_patterns:
        if re.search(pat, en_l) or re.search(pat, hi_l):
            return False

    # ‚ùå remove sentences with too many symbols
    if len(re.findall(r'[^a-zA-Z\u0900-\u097F\s]', en)) > 3:
        return False
    if len(re.findall(r'[^\u0900-\u097F\s]', hi)) > 3:
        return False

    # ‚ùå remove very short phrases
    if len(en.split()) < 3 or len(hi.split()) < 3:
        return False

    return True


# Apply cleaning
before = len(df)

df = df[df.apply(lambda x: is_clean_sentence(x['english'], x['hindi']), axis=1)]

after = len(df)

print("Removed noisy sentence pairs:", before - after)
print("Remaining clean dataset size:", after)

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
stop_words = stopwords.words('english')

import re

# Hindi (Devanagari) characters
def contains_hindi(text):
    return bool(re.search(r'[\u0900-\u097F]', str(text)))

# English (Latin) characters
def contains_english(text):
    return bool(re.search(r'[a-zA-Z]', str(text)))

df_clean = df[
    (~df["english"].apply(contains_hindi)) &   # English sentence has NO Hindi chars
    (~df["hindi"].apply(contains_english))     # Hindi sentence has NO English chars
].reset_index(drop=True)

print("Original size :", len(df))
print("Cleaned size  :", len(df_clean))

import re
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Make sure to use the correct column name for English
corpus_list = []
for line in df['english']:   # change 'english' to your actual column name
    cleaned_line = re.sub(r'[^A-Za-z\s]', '', str(line))  # keep only English letters
    cleaned_line = ' '.join(cleaned_line.split())          # remove extra spaces
    corpus_list.append(cleaned_line)

corpus = ' '.join(corpus_list)

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(corpus)

plt.figure(figsize=(13,6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

Embedding_dim=100
Latent_dim=400
max_seq_len=100
max_vocab_size=35000

# As the data size is large we will work with 20000 sentences
input_text=[]
target_text=[]
target_text_input=[]
translation=[]
i=19999
for j,t in enumerate(df['english']):
    input_text.append(t)
    if j==i:
        j=0
        break
for j,t in enumerate(df['hindi']):
    translation.append(t)
    target_text.append(t+' <eos>')
    target_text_input.append('<sos> '+t)
    if j==i:
        break

#Input
tokenizer_input=Tokenizer(num_words=max_vocab_size)
tokenizer_input.fit_on_texts(input_text)
tokenized_input=tokenizer_input.texts_to_sequences(input_text)

word2idx=tokenizer_input.word_index
print('word2idx length:',len(word2idx))

#Output
tokenizer_output=Tokenizer(num_words=max_vocab_size,filters='')
tokenizer_output.fit_on_texts(target_text_input+target_text)
tokenized_target=tokenizer_output.texts_to_sequences(target_text)              #target output
tokenized_target_input=tokenizer_output.texts_to_sequences(target_text_input)  #teacher forcing

word2idx_output=tokenizer_output.word_index
print('length of word2idx output:',len(word2idx_output))
num_words_output=len(word2idx_output)+1

import zipfile

# Extract the zip
with zipfile.ZipFile("/content/glove.6B.100d.txt.zip", 'r') as zip_ref:
    zip_ref.extractall("/content/")

# Now read the extracted file
word_vec = {}
with open("/content/glove.6B.100d.txt", encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], dtype='float32')
        word_vec[word] = vector

print('length of word vector =', len(word_vec))

#Input
encoder_input=pad_sequences(tokenized_input,maxlen=max_seq_len)
print('Encoder input shape=',encoder_input.shape)

#target output as input for teacher forcing
decoder_input=pad_sequences(tokenized_target_input,maxlen=max_seq_len,padding='post')
print('decoder input shape:',decoder_input.shape)

#target output
decoder_output=pad_sequences(tokenized_target,maxlen=max_seq_len,padding='post')
print('decoder output shape:',decoder_output.shape)

#creating count matrix
num_words=min(max_vocab_size,len(word2idx)+1)
word_embedding=np.zeros((num_words,Embedding_dim))
for word,i in word2idx.items():
    if i<max_vocab_size:
        vec=word_vec.get(word)
        if vec is not None:
            word_embedding[i]=vec
print('shape of word_embedding:',word_embedding.shape)

embedding_layer_input=Embedding(num_words,
                          Embedding_dim,
                          weights=[word_embedding],
                          trainable=True)

import  tensorflow as tf
# Encoder (attention-ready)
encoder_input_placeholder = Input(shape=(max_seq_len,))
x = embedding_layer_input(encoder_input_placeholder)

encoder_lstm = LSTM(
    Latent_dim,
    return_sequences=True,   # üî• REQUIRED for attention
    return_state=True
)

encoder_outputs, h, c = encoder_lstm(x)
encoder_states = [h, c]

# Encoder model for INFERENCE (REQUIRED because of attention)
encoder_model = Model(
    encoder_input_placeholder,
    [encoder_outputs, h, c]
)

# Decoder (with attention)
decoder_input_placeholder = Input(shape=(max_seq_len,))
decoder_embedding = Embedding(num_words_output, Embedding_dim)
x = decoder_embedding(decoder_input_placeholder)

decoder_lstm = LSTM(Latent_dim, return_state=True, return_sequences=True)
decoder_outputs, _, _ = decoder_lstm(x, initial_state=encoder_states)

# üî• ATTENTION MECHANISM
attention = tf.keras.layers.Attention()
context_vector = attention([decoder_outputs, encoder_outputs])

# üî• Combine decoder output with attention context
decoder_outputs = tf.keras.layers.Concatenate(axis=-1)(
    [decoder_outputs, context_vector]
)

decoder_dense = Dense(num_words_output, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

model=Model([encoder_input_placeholder,decoder_input_placeholder],      # Teacher Forcing
            decoder_outputs)

model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

hist=model.fit([encoder_input,decoder_input],
               decoder_output,
               epochs=250,
               validation_split=0.2,
               batch_size=32)

plt.plot(hist.history['val_accuracy'],label='val_acc')
plt.plot(hist.history['accuracy'],label='accuracy')
plt.legend()

plt.plot(hist.history['val_loss'],label='val_loss')
plt.plot(hist.history['loss'],label='loss')
plt.legend()

encoder_model=Model(encoder_input_placeholder,encoder_states)

from tensorflow.keras.layers import Concatenate

# Decoder for prediction (one step at a time)

decoder_inital_h = Input(shape=(Latent_dim,))
decoder_inital_c = Input(shape=(Latent_dim,))
decoder_initial_states = [decoder_inital_h, decoder_inital_c]

# Encoder outputs needed for attention
encoder_outputs_inf = Input(shape=(max_seq_len, Latent_dim))

decoder_input_single = Input(shape=(1,))
decoder_embedding_single = decoder_embedding(decoder_input_single)

decoder_outputs_x, h, c = decoder_lstm(
    decoder_embedding_single,
    initial_state=decoder_initial_states
)

# üî• ATTENTION (same as training)
context_vector = attention([decoder_outputs_x, encoder_outputs_inf])

# üî• Concatenate (to match training shape = 800)
decoder_combined = Concatenate(axis=-1)(
    [decoder_outputs_x, context_vector]
)

decoder_output = decoder_dense(decoder_combined)

decoder_states = [h, c]

decoder_model = Model(
    [decoder_input_single, encoder_outputs_inf] + decoder_initial_states,
    [decoder_output] + decoder_states
)

idx2word_output={v:k for k,v in word2idx_output.items()}

def sample_with_temperature(probs, temperature=0.7):
    probs = np.asarray(probs).astype("float64")
    probs = np.log(probs + 1e-9) / temperature
    probs = np.exp(probs) / np.sum(np.exp(probs))
    return np.random.choice(len(probs), p=probs)

def decode_sequence(input_seq):
    # Encode the input sequence
    encoder_outputs, h, c = encoder_model.predict(input_seq)

    # <sos> token
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = tokenizer_output.word_index['<sos>']

    decoded_sentence = []

    for _ in range(max_seq_len):
        output_tokens, h, c = decoder_model.predict(
            [target_seq, encoder_outputs, h, c]   # üî• PASS encoder_outputs
        )

        sampled_token_index = sample_with_temperature(
        output_tokens[0, -1, :], temperature=0.7
               )



        sampled_word = tokenizer_output.index_word.get(sampled_token_index, '')

        if sampled_word == '<eos>' or sampled_word == '':
            break

        decoded_sentence.append(sampled_word)

        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index

    return ' '.join(decoded_sentence)

print(len(encoder_model.outputs))
print(encoder_model.outputs)

print(len(encoder_model.outputs))
print(encoder_model.output)

i=0
while i<10:
    j=np.random.choice(len(encoder_input))
    input_seq=encoder_input[j:j+1]
    seq=decode_sequence(input_seq)
    print('------------------')
    print('input text:',input_text[j])
    print('real output',translation[j])
    print('predicted output:',seq)
    i+=1

# Number of samples to evaluate
num_samples = 50

# Randomly select indices
sample_indices = np.random.choice(len(encoder_input), num_samples, replace=False)

# Generate predictions
predicted_outputs = []
for idx in sample_indices:
    input_seq = encoder_input[idx:idx+1]
    pred = decode_sequence(input_seq)
    predicted_outputs.append(pred)

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import re

smooth = SmoothingFunction().method5

def clean_text(text):
    text = text.lower().strip()
    text = re.sub(r"[^\w\s]", "", text)
    return text

bleu_scores = []
for ref, pred in zip([translation[i] for i in sample_indices], predicted_outputs):
    ref_clean = clean_text(ref)
    pred_clean = clean_text(pred)

    bleu = sentence_bleu(
        [ref_clean.split()],
        pred_clean.split(),
        weights=(0.5, 0.5, 0, 0),
        smoothing_function=smooth
    )
    bleu_scores.append(bleu)

avg_bleu = sum(bleu_scores) / len(bleu_scores)
print(f"Average BLEU: {avg_bleu:.4f}")